from langchain_core.callbacks import BaseCallbackHandler
import streamlit as st
from scripts.graph import build_graph

# Define a custom streaming handler that updates the UI in real-time
class StreamHandler(BaseCallbackHandler):
    
    def __init__(self, container, initial_text=""):
        """
        Initialize the StreamHandler.
        
        Args:
        - container: A Streamlit container (`st.empty()`) where the text will be displayed.
        - initial_text: The starting text for the container (default is an empty string).
        """
        self.container = container  # Store the Streamlit container
        self.text = initial_text  # Initialize the text storage

    def on_llm_new_token(self, token: str, **kwargs):
        """
        Callback method triggered when a new token is generated by the LLM.
        
        Args:
        - token: The new token generated by the LLM.
        - kwargs: Additional arguments (not used here).
        """
        self.text += token  # Append the new token to the existing text
        self.container.markdown(self.text)  # Update the Streamlit UI with the latest text

def stream_graph_updates(user_query: str, thread_id: str) -> str:
    """
    Process a user query using the LangGraph workflow and stream the response.

    Args:
        user_query (str): The user's input query.
        thread_id (str): Unique identifier for the conversation thread.

    Returns:
        str: The final response from the chatbot.
    """
    graph = st.session_state.get("graph")
    if not graph:
        graph = build_graph()
        st.session_state["graph"] = graph

    config = {"configurable": {"thread_id": thread_id}}
    state = graph.invoke({"messages": [{"role": "user", "content": user_query}]}, config=config)
    response = state["messages"][-1].content
    return response